{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title-cell",
   "metadata": {},
   "source": [
    "# Classification with Scikit-Learn\n",
    "\n",
    "Comparing different classification algorithms on the BankNote Authentication dataset.\n",
    "\n",
    "**Classifiers tested:**\n",
    "- Logistic Regression\n",
    "- Support Vector Machine (SVM)\n",
    "- Decision Tree\n",
    "- Random Forest\n",
    "- K-Nearest Neighbors (KNN)\n",
    "- Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03dbe828-2a72-4f7a-9adb-70df12472bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# IMPORTS\n",
    "# ============================================\n",
    "import pandas as pd                          # Data manipulation and CSV loading\n",
    "import numpy as np                           # Numerical operations\n",
    "import seaborn as sb                         # Visualization (pairplots)\n",
    "from sklearn.metrics import confusion_matrix # Shows prediction breakdown\n",
    "from sklearn.metrics import classification_report  # Precision, recall, F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da34fc00-3cd0-4441-afd9-4261c678f2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# LOAD DATASET\n",
    "# ============================================\n",
    "# BankNote Authentication: detect forged banknotes\n",
    "# Features extracted from images using Wavelet Transform\n",
    "# Target: 0 = genuine, 1 = forged\n",
    "dataset = pd.read_csv(\"BankNote_Authentication.csv\")\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634954cd-f1a1-4ddd-95f1-ef6d77de25f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# PREPARE FEATURES AND TARGET\n",
    "# ============================================\n",
    "# X = features (first 4 columns): variance, skewness, kurtosis, entropy\n",
    "# Y = target (column 5): class label (0 or 1)\n",
    "X = dataset.iloc[:, :4]   # All rows, columns 0-3\n",
    "Y = dataset.iloc[:, 4]    # All rows, column 4 (target)\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {Y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d053fe2a-fe19-49ad-a6bf-16fd9ce3fadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# TRAIN/TEST SPLIT\n",
    "# ============================================\n",
    "# Split data: 90% training, 10% testing\n",
    "# random_state=0 ensures reproducible results\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, Y, \n",
    "    test_size=0.1,      # 10% for testing\n",
    "    random_state=0      # Seed for reproducibility\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a24adbf-0939-488a-b231-ab12b2a266d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 1. LOGISTIC REGRESSION\n",
    "# ============================================\n",
    "# Despite the name, it's a CLASSIFICATION algorithm\n",
    "# Uses sigmoid function to output probabilities (0-1)\n",
    "# Good baseline, fast, works well on linearly separable data\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train, y_train)           # Train\n",
    "predictions = classifier.predict(X_test)   # Predict\n",
    "\n",
    "print(\"LOGISTIC REGRESSION\")\n",
    "print(classification_report(y_test, predictions))\n",
    "print(confusion_matrix(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a21a11a-2c0f-436d-81ee-839c3ef1436d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 2. SUPPORT VECTOR MACHINE (SVM)\n",
    "# ============================================\n",
    "# Finds optimal hyperplane to separate classes\n",
    "# Effective in high-dimensional spaces\n",
    "# Can use kernel trick for non-linear boundaries\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "classifier = SVC()                         # Default: RBF kernel\n",
    "classifier.fit(X_train, y_train)\n",
    "predictions = classifier.predict(X_test)\n",
    "\n",
    "print(\"SUPPORT VECTOR MACHINE\")\n",
    "print(classification_report(y_test, predictions))\n",
    "print(confusion_matrix(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06863fa2-2f8d-4e61-b281-961e2ea4487f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 3. DECISION TREE\n",
    "# ============================================\n",
    "# Creates a tree of if-then rules\n",
    "# Easy to interpret and visualize\n",
    "# Prone to overfitting without pruning\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "classifier = DecisionTreeClassifier()\n",
    "classifier.fit(X_train, y_train)\n",
    "predictions = classifier.predict(X_test)\n",
    "\n",
    "print(\"DECISION TREE\")\n",
    "print(classification_report(y_test, predictions))\n",
    "print(confusion_matrix(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d2accf-378c-443d-b4dc-4d692621ec16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 4. RANDOM FOREST\n",
    "# ============================================\n",
    "# Ensemble of many decision trees\n",
    "# Each tree trained on random subset of data/features\n",
    "# Reduces overfitting, more robust than single tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "classifier = RandomForestClassifier()\n",
    "classifier.fit(X_train, y_train)\n",
    "predictions = classifier.predict(X_test)\n",
    "\n",
    "print(\"RANDOM FOREST\")\n",
    "print(classification_report(y_test, predictions))\n",
    "print(confusion_matrix(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c1543b-db6f-41bc-b883-77adde58d19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 5. K-NEAREST NEIGHBORS (KNN)\n",
    "# ============================================\n",
    "# Classifies based on K closest training examples\n",
    "# No training phase (lazy learner)\n",
    "# Simple but can be slow on large datasets\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "classifier = KNeighborsClassifier()        # Default: k=5\n",
    "classifier.fit(X_train, y_train)\n",
    "predictions = classifier.predict(X_test)\n",
    "\n",
    "print(\"K-NEAREST NEIGHBORS\")\n",
    "print(classification_report(y_test, predictions))\n",
    "print(confusion_matrix(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de7a21f-37c0-4953-b521-bb9d34357341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 6. NAIVE BAYES\n",
    "# ============================================\n",
    "# Based on Bayes' theorem\n",
    "# Assumes features are independent (\"naive\")\n",
    "# Fast, works well with high-dimensional data\n",
    "# Good for text classification\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "classifier = GaussianNB()                  # Gaussian for continuous features\n",
    "classifier.fit(X_train, y_train)\n",
    "predictions = classifier.predict(X_test)\n",
    "\n",
    "print(\"NAIVE BAYES\")\n",
    "print(classification_report(y_test, predictions))\n",
    "print(confusion_matrix(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-cell",
   "metadata": {},
   "source": [
    "## Understanding the Metrics\n",
    "\n",
    "**Precision**: Of all predicted positives, how many are actually positive?\n",
    "- High precision = few false positives\n",
    "\n",
    "**Recall**: Of all actual positives, how many did we predict?\n",
    "- High recall = few false negatives\n",
    "\n",
    "**F1-Score**: Harmonic mean of precision and recall\n",
    "- Balances both metrics\n",
    "\n",
    "**Confusion Matrix**:\n",
    "```\n",
    "[[TN  FP]\n",
    " [FN  TP]]\n",
    "```\n",
    "- TN = True Negatives (correctly predicted 0)\n",
    "- FP = False Positives (predicted 1, actually 0)\n",
    "- FN = False Negatives (predicted 0, actually 1)\n",
    "- TP = True Positives (correctly predicted 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
