{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd67a1bd-ad6b-4bb3-af74-e2ca4061883d",
   "metadata": {},
   "source": [
    "# MNIST Handwriting Recognition with TensorFlow\n",
    "https://www.youtube.com/watch?v=72s6hJwyfDg\n",
    "\n",
    "## TensorFlow vs Keras\n",
    "- **TensorFlow**: Low-level library (like NumPy for deep learning). Rich ecosystem, fine-grained control.\n",
    "- **Keras**: High-level API built ON TOP of TensorFlow. Easy-to-use abstraction layer.\n",
    "\n",
    "Think of it like: TensorFlow = engine, Keras = steering wheel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c40b2d0d-28d4-455e-8b0b-29f345abf742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.16.2\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# IMPORTS\n",
    "# ============================================\n",
    "# TensorFlow: Google's open-source deep learning framework\n",
    "# We access Keras through tf.keras (Keras is now part of TensorFlow)\n",
    "import tensorflow as tf\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3216990e-5a36-4688-bae8-e7c06c987828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# LOAD DATASET\n",
    "# ============================================\n",
    "# MNIST: Famous dataset of handwritten digits (0-9)\n",
    "# - 70,000 grayscale images, each 28x28 pixels\n",
    "# - The \"Hello World\" of machine learning\n",
    "",
    "\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "# load_data() splits into:\n",
    "# - Training set (60,000): What the model learns from\n",
    "# - Test set (10,000): What we evaluate on (model NEVER sees during training)\n",
    "# X = images (features/inputs)\n",
    "# y = labels (targets/answers)\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# NORMALIZE: Scale pixel values from 0-255 to 0-1\n",
    "# WHY? Neural networks work better with small numbers:\n",
    "# - Prevents exploding gradients\n",
    "# - Faster convergence\n",
    "# - More stable training\n",
    "X_train, X_test = X_train / 255.0, X_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6af4ba2c-01dc-4c62-b905-f543410b9f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (60000, 28, 28)\n",
      "y_train shape: (60000,)\n",
      "X_test shape: (10000, 28, 28)\n",
      "y_test shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "# Check data shapes\n",
    "print(f\"X_train shape: {X_train.shape}\")  # (60000, 28, 28) = 60k images, 28x28 pixels\n",
    "print(f\"y_train shape: {y_train.shape}\")  # (60000,) = 60k labels (integers 0-9)\n",
    "print(f\"X_test shape: {X_test.shape}\")    # (10000, 28, 28)\n",
    "print(f\"y_test shape: {y_test.shape}\")    # (10000,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8309b62d-1871-42b3-a294-d315d1ba5ec4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize a sample image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(3, 3))\n",
    "plt.imshow(X_train[0], cmap='gray')\n",
    "plt.title(f\"Label: {y_train[0]}\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b21eb2c0-71a8-4ffc-a675-041f887e82e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# BUILD MODEL / ARCHITECTURE\n",
    "# ============================================\n",
    "# Sequential: layers stack one after another (input \u2192 layer1 \u2192 layer2 \u2192 output)\n",
    "# This is the simplest way to build a neural network\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout, Input\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# INPUT LAYER: Define the shape of incoming data\n",
    "# (28, 28) = each image is 28x28 pixels\n",
    "model.add(Input((28, 28)))\n",
    "\n",
    "# FLATTEN LAYER: Reshape 2D image into 1D array\n",
    "# (28, 28) \u2192 (784,) because Dense layers need 1D input\n",
    "# No learnable parameters, just reshapes data\n",
    "model.add(Flatten())\n",
    "\n",
    "# DENSE (FULLY CONNECTED) LAYER: The \"learning\" layer\n",
    "# - 128 neurons: each connected to ALL 784 inputs\n",
    "# - Each connection has a weight (learnable parameter)\n",
    "# - activation='relu': ReLU (Rectified Linear Unit)\n",
    "#   ReLU(x) = max(0, x) \u2192 outputs x if positive, else 0\n",
    "#   WHY ReLU? Adds non-linearity so network can learn complex patterns\n",
    "#   Without activation, stacking layers = just one big linear function\n",
    "model.add(Dense(128, activation='relu'))\n",
    "\n",
    "# DROPOUT LAYER: Regularization technique\n",
    "# - Randomly \"turns off\" 20% of neurons during TRAINING\n",
    "# - Forces network to not rely on specific neurons\n",
    "# - Prevents overfitting (memorizing vs generalizing)\n",
    "# - Disabled during prediction (all neurons active)\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# OUTPUT LAYER: 10 neurons (one per digit 0-9)\n",
    "# NO SOFTMAX HERE - outputs raw scores called \"logits\"\n",
    "# WHY? More numerically stable when combined with from_logits=True in loss\n",
    "# The loss function will apply softmax internally\n",
    "model.add(Dense(10))  # Raw logits output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5df5bbbc-b8d0-435b-a340-ad71ef4e78c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw logits (nonsense because untrained):\n",
      "[[ 0.6019714  -0.2683456  -0.33950067  0.3597406  -0.31887192  0.3886709\n",
      "  -0.3568244  -0.6213937  -0.13945    -0.20903116]]\n",
      "\n",
      "Shape: (1, 10)\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# TEST UNTRAINED MODEL\n",
    "# ============================================\n",
    "# Pass one image through to see what the untrained model outputs\n",
    "# Output will be 10 \"logits\" (raw scores) - one per digit class\n",
    "\n",
    "nonsense_prediction = model(X_train[:1]).numpy()\n",
    "\n",
    "print(\"Raw logits (nonsense because untrained):\")\n",
    "print(nonsense_prediction)\n",
    "print(f\"\\nShape: {nonsense_prediction.shape}\")  # (1, 10) = 1 sample, 10 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02c55d67-074d-4b31-b6a6-53a14ea4f085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After softmax (probabilities):\n",
      "[[0.18524405 0.07758368 0.07225504 0.14539342 0.07376105 0.14966112\n",
      "  0.07101409 0.05450591 0.08825697 0.08232472]]\n",
      "\n",
      "Sum: 1.0000\n",
      "Predicted class: 0 (actual: 5)\n"
     ]
    }
   ],
   "source": [
    "# SOFTMAX: Convert logits to probabilities\n",
    "# - Makes outputs more interpretable (0-1 range, sum to 1)\n",
    "# - softmax(x_i) = e^x_i / sum(e^x_j)\n",
    "# - Highest logit \u2192 highest probability\n",
    "\n",
    "probabilities = tf.nn.softmax(nonsense_prediction).numpy()\n",
    "print(\"After softmax (probabilities):\")\n",
    "print(probabilities)\n",
    "print(f\"\\nSum: {probabilities.sum():.4f}\")  # Should be 1.0\n",
    "print(f\"Predicted class: {probabilities.argmax()} (actual: {y_train[0]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be00aa97-9ddc-4cba-a4b6-cd5fcd0528bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected loss (random guess): 2.3026\n",
      "Actual loss (untrained model): 1.8994\n",
      "(Close to expected = model is randomly guessing, as expected!)\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# LOSS FUNCTION\n",
    "# ============================================\n",
    "# Loss = how wrong the model is (lower = better)\n",
    "# Training goal: minimize this number\n",
    "\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "\n",
    "# SparseCategoricalCrossentropy breakdown:\n",
    "# - \"Sparse\": Labels are integers (5, 3, 7...) not one-hot ([0,0,0,0,0,1,0,0,0,0])\n",
    "# - \"Categorical\": Multi-class classification (more than 2 classes)\n",
    "# - \"CrossEntropy\": Mathematical formula comparing probability distributions\n",
    "#\n",
    "# from_logits=True: CRITICAL!\n",
    "# - Tells loss function our model outputs RAW LOGITS, not probabilities\n",
    "# - Loss function applies softmax internally\n",
    "# - WHY? More numerically stable (avoids log(0) errors)\n",
    "\n",
    "loss_fn = SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# Expected loss for random guessing:\n",
    "# - 10 classes = 1/10 = 10% chance of being right\n",
    "# - Cross-entropy loss = -log(probability)\n",
    "# - -log(1/10) \u2248 2.3\n",
    "expected_loss = -tf.math.log(1/10)\n",
    "actual_loss = loss_fn(y_train[:1], nonsense_prediction)\n",
    "\n",
    "print(f\"Expected loss (random guess): {expected_loss.numpy():.4f}\")\n",
    "print(f\"Actual loss (untrained model): {actual_loss.numpy():.4f}\")\n",
    "print(\"(Close to expected = model is randomly guessing, as expected!)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90cac699-0a78-46ad-b2a5-58aaa4c79bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# COMPILE MODEL\n",
    "# ============================================\n",
    "# Compile = configure HOW the model will learn\n",
    "# Must be done before training\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# OPTIMIZER: Algorithm that updates weights to minimize loss\n",
    "# Adam (Adaptive Moment Estimation):\n",
    "# - Most popular optimizer, works well in most cases\n",
    "# - Adapts learning rate for each parameter\n",
    "# - Combines momentum + RMSprop benefits\n",
    "#\n",
    "# learning_rate: How big each weight update is\n",
    "# - Too high (0.1): Overshoots, unstable, loss may explode\n",
    "# - Too low (0.0001): Very slow training\n",
    "# - 0.01: Aggressive but often works well\n",
    "# - 0.001: Default, safer choice\n",
    "optimizer = Adam(learning_rate=0.01)\n",
    "\n",
    "# model.compile() parameters:\n",
    "# - optimizer: How to update weights\n",
    "# - loss: What to minimize (our cross-entropy function)\n",
    "# - metrics: What to display during training (accuracy %)\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=loss_fn,  # SparseCategoricalCrossentropy(from_logits=True)\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b92582bb-37ca-48dc-b926-23cd85727e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 7ms/step - accuracy: 0.8078 - loss: 2.0411\n",
      "Epoch 2/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 7ms/step - accuracy: 0.8248 - loss: 4.0101\n",
      "Epoch 3/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 7ms/step - accuracy: 0.8271 - loss: 5.6564\n",
      "Epoch 4/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 7ms/step - accuracy: 0.8280 - loss: 6.9391\n",
      "Epoch 5/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 7ms/step - accuracy: 0.8296 - loss: 8.3890\n",
      "Epoch 6/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 7ms/step - accuracy: 0.8280 - loss: 9.6539\n",
      "Epoch 7/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 7ms/step - accuracy: 0.8295 - loss: 10.7206\n",
      "Epoch 8/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 7ms/step - accuracy: 0.8314 - loss: 12.1751\n",
      "Epoch 9/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 7ms/step - accuracy: 0.8302 - loss: 13.4741\n",
      "Epoch 10/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 7ms/step - accuracy: 0.8314 - loss: 14.7291\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x360ee4bf0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================\n",
    "# TRAIN MODEL\n",
    "# ============================================\n",
    "# model.fit() = where the actual learning happens!\n",
    "#\n",
    "# What happens each EPOCH (one pass through all training data):\n",
    "# 1. FORWARD PASS: Images flow through network \u2192 predictions\n",
    "# 2. CALCULATE LOSS: Compare predictions to actual labels\n",
    "# 3. BACKWARD PASS (Backpropagation): Calculate gradients\n",
    "#    - How much did each weight contribute to the error?\n",
    "# 4. UPDATE WEIGHTS: Optimizer adjusts weights to reduce loss\n",
    "#\n",
    "# epochs=10: Go through ALL 60,000 images 10 times\n",
    "# More epochs = more learning (but risk overfitting)\n",
    "#\n",
    "# WATCH FOR:\n",
    "# - loss should DECREASE (model getting better)\n",
    "# - accuracy should INCREASE (more correct predictions)\n",
    "\n",
    "model.fit(X_train, y_train, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a98c824-baee-4997-b283-6d8a52d3e9cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8556 - loss: 13.1936\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[13.193587303161621, 0.8555999994277954]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================\n",
    "# EVALUATE MODEL\n",
    "# ============================================\n",
    "# Test on data the model has NEVER seen during training\n",
    "# This shows how well the model generalizes to new data\n",
    "#\n",
    "# Returns: [loss, accuracy]\n",
    "# - If test accuracy << training accuracy \u2192 overfitting\n",
    "# - If test accuracy \u2248 training accuracy \u2192 good generalization\n",
    "\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a5551965-511f-4c45-9fde-7c85ea45afed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: 7\n",
      "Actual: 7\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# MAKE PREDICTIONS\n",
    "# ============================================\n",
    "# Use the trained model to predict on new data\n",
    "\n",
    "# Get prediction for first test image\n",
    "prediction_logits = model(X_test[:1])\n",
    "\n",
    "# Convert logits \u2192 probabilities \u2192 class\n",
    "predicted_class = tf.argmax(tf.nn.softmax(prediction_logits), axis=1)\n",
    "\n",
    "print(f\"Predicted: {predicted_class.numpy()[0]}\")\n",
    "print(f\"Actual: {y_test[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ktuenqlab",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "Systematically test different model configurations to find the best one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "q96he8qm26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Config 1/5: {'neurons': 32, 'dropout': 0.1, 'lr': 0.001} ---\n",
      "Accuracy: 0.9173, Loss: 0.3019\n",
      "\n",
      "--- Config 2/5: {'neurons': 64, 'dropout': 0.2, 'lr': 0.001} ---\n"
     ]
    }
   ],
   "source": [
    "# Manual Hyperparameter Comparison\n",
    "# Test different configurations and compare results\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Input, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Configurations to test\n",
    "# Each dict defines a different model architecture\n",
    "configs = [\n",
    "    {'neurons': 32,  'dropout': 0.1, 'lr': 0.001},\n",
    "    {'neurons': 64,  'dropout': 0.2, 'lr': 0.001},\n",
    "    {'neurons': 128, 'dropout': 0.2, 'lr': 0.001},\n",
    "    {'neurons': 128, 'dropout': 0.3, 'lr': 0.01},\n",
    "    {'neurons': 256, 'dropout': 0.3, 'lr': 0.001},\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "for i, config in enumerate(configs):\n",
    "    print(f\"\\n--- Config {i+1}/{len(configs)}: {config} ---\")\n",
    "    \n",
    "    # Build model with this config\n",
    "    model = Sequential([\n",
    "        Input((28, 28)),\n",
    "        Flatten(),\n",
    "        Dense(config['neurons'], activation='relu'),\n",
    "        Dropout(config['dropout']),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=config['lr']),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # Train (fewer epochs for speed)\n",
    "    model.fit(X_train, y_train, epochs=5, verbose=0)\n",
    "    \n",
    "    # Evaluate\n",
    "    loss, acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "    \n",
    "    results.append({**config, 'accuracy': acc, 'loss': loss})\n",
    "    print(f\"Accuracy: {acc:.4f}, Loss: {loss:.4f}\")\n",
    "\n",
    "# Find best configuration\n",
    "best = max(results, key=lambda x: x['accuracy'])\n",
    "print(f\"\\n\u2713 Best config: {best}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nnx463jsv8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results as a table\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "df = df.sort_values('accuracy', ascending=False)\n",
    "df['accuracy'] = df['accuracy'].apply(lambda x: f\"{x:.4f}\")\n",
    "df['loss'] = df['loss'].apply(lambda x: f\"{x:.4f}\")\n",
    "print(\"Results sorted by accuracy:\\n\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eyv0mqaqi6",
   "metadata": {},
   "source": [
    "### Keras Tuner (Automated Search)\n",
    "More powerful automated hyperparameter search. Install with: `pip install keras-tuner`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eazeb9pb0ka",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install keras-tuner (run once)\n",
    "# !pip install keras-tuner\n",
    "\n",
    "import keras_tuner as kt\n",
    "\n",
    "def build_model(hp):\n",
    "    \"\"\"\n",
    "    Build model with tunable hyperparameters.\n",
    "    hp = hyperparameter object that lets us define search ranges\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Input((28, 28)))\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    # Tune: number of neurons in first layer (choices: 32, 64, 128, 256)\n",
    "    model.add(Dense(\n",
    "        units=hp.Choice('neurons', values=[32, 64, 128, 256]),\n",
    "        activation='relu'\n",
    "    ))\n",
    "    \n",
    "    # Tune: dropout rate (range: 0.1 to 0.5, step 0.1)\n",
    "    model.add(Dropout(hp.Float('dropout', min_value=0.1, max_value=0.5, step=0.1)))\n",
    "    \n",
    "    # Tune: optional second hidden layer\n",
    "    if hp.Boolean('second_layer'):\n",
    "        model.add(Dense(64, activation='relu'))\n",
    "        model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    \n",
    "    # Tune: learning rate\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=hp.Choice('learning_rate', values=[0.01, 0.001, 0.0001])),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create tuner - RandomSearch tries random combinations\n",
    "tuner = kt.RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_accuracy',      # What to optimize\n",
    "    max_trials=10,                 # Number of different configs to try\n",
    "    executions_per_trial=1,        # Train each config once\n",
    "    directory='tuner_results',     # Where to save results\n",
    "    project_name='mnist_tuning'\n",
    ")\n",
    "\n",
    "# Show search space\n",
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lhxjgdr786j",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the hyperparameter search\n",
    "# This will train 10 different model configurations\n",
    "tuner.search(\n",
    "    X_train, y_train,\n",
    "    epochs=5,\n",
    "    validation_data=(X_test, y_test),\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fuc4bgw5zif",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best hyperparameters\n",
    "best_hp = tuner.get_best_hyperparameters()[0]\n",
    "\n",
    "print(\"Best Hyperparameters Found:\")\n",
    "print(f\"  - Neurons: {best_hp.get('neurons')}\")\n",
    "print(f\"  - Dropout: {best_hp.get('dropout')}\")\n",
    "print(f\"  - Learning Rate: {best_hp.get('learning_rate')}\")\n",
    "print(f\"  - Second Layer: {best_hp.get('second_layer')}\")\n",
    "\n",
    "# Show top 3 results\n",
    "print(\"\\nTop 3 trials:\")\n",
    "tuner.results_summary(num_trials=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tudf49t5iw",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the best model and train with more epochs\n",
    "best_model = tuner.hypermodel.build(best_hp)\n",
    "\n",
    "# Train the best model for longer\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# EarlyStopping: stop training if no improvement for 3 epochs\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=3,\n",
    "    restore_best_weights=True  # Keep the best weights\n",
    ")\n",
    "\n",
    "print(\"Training best model with more epochs...\")\n",
    "best_model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=20,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=[early_stop]\n",
    ")\n",
    "\n",
    "# Final evaluation\n",
    "loss, accuracy = best_model.evaluate(X_test, y_test)\n",
    "print(f\"\\n\u2713 Final Test Accuracy: {accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12 (TensorFlow GPU)",
   "language": "python",
   "name": "python312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}